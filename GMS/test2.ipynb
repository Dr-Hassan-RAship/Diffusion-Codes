{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from networks.novel.lite_vae.blocks.haar_own import HaarTransform\n",
    "import torch\n",
    "# from networks.novel.lite_vae.blocks.smc         import SMC\n",
    "# from networks.novel.lite_vae.blocks.resblock    import ResBlock, ResBlockWithSMC\n",
    "# from networks.novel.lite_vae.blocks.midblock    import MidBlock2D\n",
    "# from networks.novel.lite_vae.blocks.unet_block  import LiteVAEUNetBlock\n",
    "from networks.novel.lite_vae.encoder            import LiteVAEEncoder\n",
    "from networks.novel.lite_vae.decoder            import *  # or SDVAEDecoder\n",
    "from networks.novel.lite_vae.litevae            import LiteVAE\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "# from typing import Dict, List\n",
    "# import matplotlib.pyplot as plt\n",
    "from data.image_dataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "# from PIL import Image\n",
    "# import os\n",
    "import ptwt, pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate grayscale images\n",
    "haar = HaarTransform(levels=3)\n",
    "\n",
    "# img_1 = Image.open(os.path.join('/media/ee/DATA/Talha_Nehal/Diffusion-Codes/GMS/Dataset/busi/images/benign_1.png')).convert(\"RGB\")\n",
    "# img_2 = Image.open(os.path.join('/media/ee/DATA/Talha_Nehal/Diffusion-Codes/GMS/Dataset/busi/images/benign_2.png')).convert(\"RGB\")\n",
    "\n",
    "# # stack them on the batch dimension and convert to tensor (B, C, H, W)\n",
    "# img = torch.stack([torch.tensor(np.array(img_1).transpose(2, 0, 1)), \n",
    "#                    torch.tensor(np.array(img_2).transpose(2, 0, 1))])\n",
    "# img.shape\n",
    "\n",
    "train_dataset = Image_Dataset('Dataset/busi/busi_train_test_names.pkl', stage=\"train\", excel = False)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=24,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = batch['img']\n",
    "img_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon, haar_transform_out_list, haar_transform_out_dict = haar(img_batch, inverse = True, stacked_version=True, visualize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet=pywt.Wavelet('haar')\n",
    "def dwt(x, level=None):\n",
    "\n",
    "    level = level or level\n",
    "    x_low, *x_high = ptwt.wavedec2(\n",
    "    x.float(),\n",
    "    wavelet=wavelet,\n",
    "    level=level,\n",
    "    mode='symmetric',\n",
    "    )\n",
    "    x_combined = torch.cat(\n",
    "    [x_low, x_high[0][0], x_high[0][1], x_high[0][2]], dim=1\n",
    "    )\n",
    "    return x_combined\n",
    "    \n",
    "def idwt(self, x):\n",
    "    x_low, x_high = x[:, :3], x[:, 3:]\n",
    "    x_high = torch.chunk(x_high, 3, dim=1)\n",
    "    x_recon = ptwt.waverec2([x_low.float(), x_high.float()], wavelet=wavelet)\n",
    "    return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute MSE of reconstructed image and original image\n",
    "mse = torch.nn.functional.mse_loss(recon, img_batch)\n",
    "print(f'MSE: {mse.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse * 224 * 224 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon[0, :].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwt_L1 = dwt(img_batch, level=1) / 2\n",
    "dwt_L2 = dwt(img_batch, level=2) / 4\n",
    "dwt_L3 = dwt(img_batch, level=3) / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwt_L1.shape, dwt_L2.shape, dwt_L3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_low, x_high, wavelet_recon = dwt(recon, level = 1)\n",
    "wavelet_recon = wavelet_recon / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_low.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet_recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_transform_out_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute MSE of reconstructed image and original image\n",
    "mse = torch.nn.functional.mse_loss(wavelet_recon, haar_transform_out_list[0])\n",
    "print(f'MSE: {mse.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharbonnierLoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-3):\n",
    "        super(CharbonnierLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        diff = prediction - target\n",
    "        loss = torch.mean(torch.sqrt(diff * diff + self.epsilon**2))\n",
    "        return loss\n",
    "\n",
    "criterion = CharbonnierLoss(epsilon=1e-3)\n",
    "loss = criterion(wavelet_recon, haar_transform_out_list[0])\n",
    "# print(f\"Charbonnier Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderTiny\n",
    "\n",
    "vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\", torch_dtype=torch.float32, device_map = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = vae.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze decoder and count the number of learnable parameters before and after\n",
    "\n",
    "learnable_params_before = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "for param in decoder.parameters():\n",
    "    param.requires_grad = False\n",
    "learnable_params_after = sum(p.numel() for p in decoder.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnable_params_before, learnable_params_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(decoder, input_size = (2, 4, 28, 28), depth = 2, col_names = (\"kernel_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LiteVAE(encoder = LiteVAEEncoder(), decoder = decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_batch = base_model(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_batch['latent'].shape, latent_batch['latent'].device, latent_batch['latent'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(base_model, input_size = (2, 3, 224, 224), depth = 2, col_names = (\"kernel_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
