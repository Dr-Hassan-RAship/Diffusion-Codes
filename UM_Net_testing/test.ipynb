{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 64, 32, 32])\n",
      "Output shape: torch.Size([2, 64, 32, 32])\n",
      "Attention weights shape: torch.Size([2, 64])\n",
      "Sample attention weights for first sample:\n",
      "tensor([0.4997, 0.5000, 0.5000, 0.5002, 0.4997])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define SELayer (as provided)\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# Create a sample input tensor\n",
    "batch_size, channels, height, width = 2, 64, 32, 32\n",
    "x = torch.randn(batch_size, channels, height, width)  # Simulated feature map\n",
    "\n",
    "# Instantiate SELayer\n",
    "se_layer = SELayer(channel=channels, reduction=16)\n",
    "\n",
    "# Forward pass\n",
    "output = se_layer(x)\n",
    "\n",
    "# Print shapes and sample attention weights\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Extract attention weights for inspection\n",
    "with torch.no_grad():\n",
    "    y = se_layer.avg_pool(x).view(batch_size, channels)\n",
    "    y = se_layer.fc(y)\n",
    "    print(f\"Attention weights shape: {y.shape}\")\n",
    "    print(f\"Sample attention weights for first sample:\\n{y[0][:5]}\")  # First 5 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 64, 32, 32])\n",
      "Output shape: torch.Size([2, 64, 32, 32])\n",
      "Attention map shape: torch.Size([2, 1024, 256])\n",
      "Sample attention weights (first 5x5 for first sample):\n",
      "tensor([[0.0052, 0.0056, 0.0017, 0.0021, 0.0009],\n",
      "        [0.0016, 0.0070, 0.0040, 0.0005, 0.0004],\n",
      "        [0.0097, 0.0216, 0.0051, 0.0020, 0.0051],\n",
      "        [0.0087, 0.0036, 0.0016, 0.0019, 0.0107],\n",
      "        [0.0001, 0.0020, 0.0030, 0.0100, 0.0002]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define NonLocalBlock (as provided)\n",
    "class NonLocalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
    "        super(NonLocalBlock, self).__init__()\n",
    "        self.sub_sample = sub_sample\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "        self.g = nn.Conv2d(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "        if bn_layer:\n",
    "            self.W = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                          kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(self.in_channels)\n",
    "            )\n",
    "            nn.init.constant_(self.W[1].weight, 0)\n",
    "            nn.init.constant_(self.W[1].bias, 0)\n",
    "        else:\n",
    "            self.W = nn.Conv2d(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                               kernel_size=1, stride=1, padding=0)\n",
    "            nn.init.constant_(self.W.weight, 0)\n",
    "            nn.init.constant_(self.W.bias, 0)\n",
    "        self.theta = nn.Conv2d(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                               kernel_size=1, stride=1, padding=0)\n",
    "        self.phi = nn.Conv2d(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "        if sub_sample:\n",
    "            self.g = nn.Sequential(self.g, nn.MaxPool2d(kernel_size=(2, 2)))\n",
    "            self.phi = nn.Sequential(self.phi, nn.MaxPool2d(kernel_size=(2, 2)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1)\n",
    "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "        f = torch.matmul(theta_x, phi_x)\n",
    "        f_div_C = F.softmax(f, dim=-1)\n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x\n",
    "        return z\n",
    "\n",
    "# Create a sample input tensor\n",
    "batch_size, in_channels, height, width = 2, 64, 32, 32\n",
    "x = torch.randn(batch_size, in_channels, height, width)  # Simulated feature map\n",
    "\n",
    "# Instantiate NonLocalBlock\n",
    "non_local_block = NonLocalBlock(in_channels=in_channels, inter_channels=None, sub_sample=True, bn_layer=True)\n",
    "\n",
    "# Forward pass\n",
    "output = non_local_block(x)\n",
    "\n",
    "# Print shapes and sample attention weights\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Extract attention map for inspection\n",
    "with torch.no_grad():\n",
    "    g_x = non_local_block.g(x).view(batch_size, non_local_block.inter_channels, -1)\n",
    "    theta_x = non_local_block.theta(x).view(batch_size, non_local_block.inter_channels, -1).permute(0, 2, 1)\n",
    "    phi_x = non_local_block.phi(x).view(batch_size, non_local_block.inter_channels, -1)\n",
    "    f = torch.matmul(theta_x, phi_x)\n",
    "    f_div_C = F.softmax(f, dim=-1)\n",
    "    print(f\"Attention map shape: {f_div_C.shape}\")\n",
    "    print(f\"Sample attention weights (first 5x5 for first sample):\\n{f_div_C[0, :5, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ee/anaconda3/envs/TaN2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, copy\n",
    "\n",
    "import torch.nn                              as nn\n",
    "\n",
    "from   diffusers                             import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n",
    "from   tqdm.auto                             import tqdm\n",
    "from   diffusers.models.autoencoders.vae     import DiagonalGaussianDistribution\n",
    "from   diffusers.models.modeling_outputs     import AutoencoderKLOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hybrid_unet(pretrained_path: str, device: str = \"cuda\") -> UNet2DConditionModel:\n",
    "\n",
    "    # Step 1: Load the original model to access weights and config\n",
    "    unet_orig = UNet2DConditionModel.from_pretrained(\n",
    "        pretrained_path,\n",
    "        subfolder=\"unet\",\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "    orig_sd = unet_orig.state_dict()\n",
    "    \n",
    "    # Step 2: Create a deep copy of the config and modify in_channels\n",
    "    config = copy.deepcopy(unet_orig.config)  # Avoid modifying the original config\n",
    "    config['in_channels'] = 8  # Update in_channels to 8\n",
    "\n",
    "    del unet_orig  # Free memory after loading original model\n",
    "    \n",
    "    # Step 3: Create a new UNet model with modified in_channels\n",
    "    unet_new = UNet2DConditionModel(**config).to(device, dtype=torch.float16)\n",
    "\n",
    "    # Step 4: Get the original and new state dicts\n",
    "\n",
    "    new_sd = unet_new.state_dict()\n",
    "\n",
    "    # Step 5: Initialize new conv_in weights manually\n",
    "    old_conv = orig_sd[\"conv_in.weight\"]  # Shape [320, 4, 3, 3]\n",
    "    out_ch, _, kH, kW = old_conv.shape\n",
    "\n",
    "    # New conv_in.weight: [320, 8, 3, 3]\n",
    "    new_conv = torch.zeros((out_ch, 8, kH, kW), dtype=torch.float16, device=device)\n",
    "    new_conv[:, :4, :, :] = old_conv  # Copy pretrained channels\n",
    "    nn.init.kaiming_normal_(new_conv[:, 4:, :, :], mode='fan_out', nonlinearity='leaky_relu')  # Random init rest\n",
    "\n",
    "    # Step 6: Update the state dict with the new conv_in.weight\n",
    "    new_sd[\"conv_in.weight\"] = new_conv\n",
    "\n",
    "    # Step 7: Copy compatible weights from the original model\n",
    "    for key in new_sd:\n",
    "        if key != \"conv_in.weight\" and key in orig_sd and new_sd[key].shape == orig_sd[key].shape:\n",
    "            new_sd[key] = orig_sd[key]\n",
    "\n",
    "    # Step 8: Load updated state dict into the new model\n",
    "    unet_new.load_state_dict(new_sd)\n",
    "\n",
    "    return config, unet_new.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config, unet = load_hybrid_unet(\"CompVis/stable-diffusion-v1-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict([('sample_size', 64),\n",
       "            ('in_channels', 4),\n",
       "            ('out_channels', 4),\n",
       "            ('center_input_sample', False),\n",
       "            ('flip_sin_to_cos', True),\n",
       "            ('freq_shift', 0),\n",
       "            ('down_block_types',\n",
       "             ['CrossAttnDownBlock2D',\n",
       "              'CrossAttnDownBlock2D',\n",
       "              'CrossAttnDownBlock2D',\n",
       "              'DownBlock2D']),\n",
       "            ('mid_block_type', 'UNetMidBlock2DCrossAttn'),\n",
       "            ('up_block_types',\n",
       "             ['UpBlock2D',\n",
       "              'CrossAttnUpBlock2D',\n",
       "              'CrossAttnUpBlock2D',\n",
       "              'CrossAttnUpBlock2D']),\n",
       "            ('only_cross_attention', False),\n",
       "            ('block_out_channels', [320, 640, 1280, 1280]),\n",
       "            ('layers_per_block', 2),\n",
       "            ('downsample_padding', 1),\n",
       "            ('mid_block_scale_factor', 1),\n",
       "            ('dropout', 0.0),\n",
       "            ('act_fn', 'silu'),\n",
       "            ('norm_num_groups', 32),\n",
       "            ('norm_eps', 1e-05),\n",
       "            ('cross_attention_dim', 768),\n",
       "            ('transformer_layers_per_block', 1),\n",
       "            ('reverse_transformer_layers_per_block', None),\n",
       "            ('encoder_hid_dim', None),\n",
       "            ('encoder_hid_dim_type', None),\n",
       "            ('attention_head_dim', 8),\n",
       "            ('num_attention_heads', None),\n",
       "            ('dual_cross_attention', False),\n",
       "            ('use_linear_projection', False),\n",
       "            ('class_embed_type', None),\n",
       "            ('addition_embed_type', None),\n",
       "            ('addition_time_embed_dim', None),\n",
       "            ('num_class_embeds', None),\n",
       "            ('upcast_attention', False),\n",
       "            ('resnet_time_scale_shift', 'default'),\n",
       "            ('resnet_skip_time_act', False),\n",
       "            ('resnet_out_scale_factor', 1.0),\n",
       "            ('time_embedding_type', 'positional'),\n",
       "            ('time_embedding_dim', None),\n",
       "            ('time_embedding_act_fn', None),\n",
       "            ('timestep_post_act', None),\n",
       "            ('time_cond_proj_dim', None),\n",
       "            ('conv_in_kernel', 3),\n",
       "            ('conv_out_kernel', 3),\n",
       "            ('projection_class_embeddings_input_dim', None),\n",
       "            ('attention_type', 'default'),\n",
       "            ('class_embeddings_concat', False),\n",
       "            ('mid_block_only_cross_attention', None),\n",
       "            ('cross_attention_norm', None),\n",
       "            ('addition_embed_type_num_heads', 64),\n",
       "            ('_use_default_values',\n",
       "             ['transformer_layers_per_block',\n",
       "              'resnet_skip_time_act',\n",
       "              'num_attention_heads',\n",
       "              'dual_cross_attention',\n",
       "              'cross_attention_norm',\n",
       "              'encoder_hid_dim_type',\n",
       "              'resnet_time_scale_shift',\n",
       "              'addition_time_embed_dim',\n",
       "              'attention_type',\n",
       "              'mid_block_type',\n",
       "              'time_cond_proj_dim',\n",
       "              'conv_out_kernel',\n",
       "              'time_embedding_type',\n",
       "              'encoder_hid_dim',\n",
       "              'use_linear_projection',\n",
       "              'mid_block_only_cross_attention',\n",
       "              'upcast_attention',\n",
       "              'time_embedding_act_fn',\n",
       "              'addition_embed_type',\n",
       "              'num_class_embeds',\n",
       "              'class_embeddings_concat',\n",
       "              'addition_embed_type_num_heads',\n",
       "              'class_embed_type',\n",
       "              'resnet_out_scale_factor',\n",
       "              'projection_class_embeddings_input_dim',\n",
       "              'reverse_transformer_layers_per_block',\n",
       "              'time_embedding_dim',\n",
       "              'dropout',\n",
       "              'conv_in_kernel',\n",
       "              'only_cross_attention',\n",
       "              'timestep_post_act']),\n",
       "            ('_class_name', 'UNet2DConditionModel'),\n",
       "            ('_diffusers_version', '0.2.2'),\n",
       "            ('_name_or_path', 'CompVis/stable-diffusion-v1-4')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict([('sample_size', 64),\n",
       "            ('in_channels', 8),\n",
       "            ('out_channels', 4),\n",
       "            ('center_input_sample', False),\n",
       "            ('flip_sin_to_cos', True),\n",
       "            ('freq_shift', 0),\n",
       "            ('down_block_types',\n",
       "             ['CrossAttnDownBlock2D',\n",
       "              'CrossAttnDownBlock2D',\n",
       "              'CrossAttnDownBlock2D',\n",
       "              'DownBlock2D']),\n",
       "            ('mid_block_type', 'UNetMidBlock2DCrossAttn'),\n",
       "            ('up_block_types',\n",
       "             ['UpBlock2D',\n",
       "              'CrossAttnUpBlock2D',\n",
       "              'CrossAttnUpBlock2D',\n",
       "              'CrossAttnUpBlock2D']),\n",
       "            ('only_cross_attention', False),\n",
       "            ('block_out_channels', [320, 640, 1280, 1280]),\n",
       "            ('layers_per_block', 2),\n",
       "            ('downsample_padding', 1),\n",
       "            ('mid_block_scale_factor', 1),\n",
       "            ('dropout', 0.0),\n",
       "            ('act_fn', 'silu'),\n",
       "            ('norm_num_groups', 32),\n",
       "            ('norm_eps', 1e-05),\n",
       "            ('cross_attention_dim', 768),\n",
       "            ('transformer_layers_per_block', 1),\n",
       "            ('reverse_transformer_layers_per_block', None),\n",
       "            ('encoder_hid_dim', None),\n",
       "            ('encoder_hid_dim_type', None),\n",
       "            ('attention_head_dim', 8),\n",
       "            ('num_attention_heads', None),\n",
       "            ('dual_cross_attention', False),\n",
       "            ('use_linear_projection', False),\n",
       "            ('class_embed_type', None),\n",
       "            ('addition_embed_type', None),\n",
       "            ('addition_time_embed_dim', None),\n",
       "            ('num_class_embeds', None),\n",
       "            ('upcast_attention', False),\n",
       "            ('resnet_time_scale_shift', 'default'),\n",
       "            ('resnet_skip_time_act', False),\n",
       "            ('resnet_out_scale_factor', 1.0),\n",
       "            ('time_embedding_type', 'positional'),\n",
       "            ('time_embedding_dim', None),\n",
       "            ('time_embedding_act_fn', None),\n",
       "            ('timestep_post_act', None),\n",
       "            ('time_cond_proj_dim', None),\n",
       "            ('conv_in_kernel', 3),\n",
       "            ('conv_out_kernel', 3),\n",
       "            ('projection_class_embeddings_input_dim', None),\n",
       "            ('attention_type', 'default'),\n",
       "            ('class_embeddings_concat', False),\n",
       "            ('mid_block_only_cross_attention', None),\n",
       "            ('cross_attention_norm', None),\n",
       "            ('addition_embed_type_num_heads', 64),\n",
       "            ('_use_default_values',\n",
       "             ['reverse_transformer_layers_per_block',\n",
       "              'resnet_time_scale_shift',\n",
       "              'class_embeddings_concat',\n",
       "              'mid_block_only_cross_attention',\n",
       "              'use_linear_projection',\n",
       "              'time_cond_proj_dim',\n",
       "              'time_embedding_dim',\n",
       "              'encoder_hid_dim_type',\n",
       "              'encoder_hid_dim',\n",
       "              'num_class_embeds',\n",
       "              'timestep_post_act',\n",
       "              'time_embedding_type',\n",
       "              'time_embedding_act_fn',\n",
       "              'dropout',\n",
       "              'num_attention_heads',\n",
       "              'cross_attention_norm',\n",
       "              'attention_type',\n",
       "              'resnet_skip_time_act',\n",
       "              'conv_in_kernel',\n",
       "              'upcast_attention',\n",
       "              'dual_cross_attention',\n",
       "              'resnet_out_scale_factor',\n",
       "              'only_cross_attention',\n",
       "              'transformer_layers_per_block',\n",
       "              'conv_out_kernel',\n",
       "              'addition_embed_type',\n",
       "              'projection_class_embeddings_input_dim',\n",
       "              'addition_time_embed_dim',\n",
       "              'addition_embed_type_num_heads',\n",
       "              'class_embed_type',\n",
       "              'mid_block_type']),\n",
       "            ('_class_name', 'UNet2DConditionModel'),\n",
       "            ('_diffusers_version', '0.2.2'),\n",
       "            ('_name_or_path', 'CompVis/stable-diffusion-v1-4')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TaN2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
