{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, logging, os, sys, torch, time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.amp                import autocast, GradScaler\n",
    "from torch.utils.tensorboard  import SummaryWriter\n",
    "from tqdm                     import tqdm\n",
    "from monai.networks.nets      import AutoencoderKL\n",
    "from generative.networks.nets import PatchDiscriminator\n",
    "from monai.losses             import PatchAdversarialLoss, PerceptualLoss\n",
    "from torch.nn                 import MSELoss\n",
    "from config_ldm_ddpm          import *\n",
    "from dataset                  import *\n",
    "from utils                    import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, shutil\n",
    "import numpy as np\n",
    "\n",
    "from torchvision        import transforms\n",
    "from torch.utils.data   import DataLoader, Dataset\n",
    "from PIL                import Image, ImageEnhance\n",
    "from skimage.morphology import disk, erosion, dilation, opening, closing\n",
    "from custom_transforms  import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Talha\\\\OneDrive - Higher Education Commission\\\\Desktop\\\\Dr. Hassan Summer Work\\\\Datasets\\\\Kvasir-SEG',\n",
       " (600, 200, 200))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('C:/Users/Talha/OneDrive - Higher Education Commission/Desktop/Dr. Hassan Summer Work/Datasets/Kvasir-SEG')\n",
    "base_dir = os.getcwd()\n",
    "split_ratios = (600, 200, 200)\n",
    "\n",
    "base_dir, split_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already split into train, val and test directories\n",
      "Dataset already split into train, val and test directories\n"
     ]
    }
   ],
   "source": [
    "# Dataloaders\n",
    "train_loader  = get_dataloaders(\n",
    "    base_dir, split_ratio=SPLIT_RATIOS, split=\"train\", \n",
    "    trainsize=TRAINSIZE, batch_size=BATCH_SIZE, format=FORMAT\n",
    ")\n",
    "val_loader    = get_dataloaders(\n",
    "    base_dir, split_ratio=SPLIT_RATIOS, split=\"val\", \n",
    "    trainsize=TRAINSIZE, batch_size=BATCH_SIZE, format=FORMAT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1, 256, 256]), torch.Size([4, 1, 256, 256]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_mask, noisy_mask  = batch['clean_mask'], batch['noisy_mask']\n",
    "clean_mask.shape, noisy_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 256, 256]), torch.Size([4, 3, 256, 256]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_image, noisy_image = batch['clean_image'], batch['noisy_image']\n",
    "clean_image.shape, noisy_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAE_IMAGE_PARAMS       = {\"spatial_dims\"              : 2,\n",
    "                          \"in_channels\"               : 3,\n",
    "                          \"latent_channels\"           : 4, # (= Z in SDSeg paper)\n",
    "                          \"out_channels\"              : 3,\n",
    "                          \"channels\"                  : (128, 256, 512, 512),\n",
    "                          \"num_res_blocks\"            : 2,\n",
    "                          \"attention_levels\"          : (False, False, False, False),\n",
    "                          \"with_encoder_nonlocal_attn\": True, # (as per SDSeg paper to ensure middle block of encoder is as required)\n",
    "                          \"with_decoder_nonlocal_attn\": True, # (as per SDSeg paper to ensure middle block of decoder is as required)\n",
    "                          \"use_flash_attention\"       : True}\n",
    "\n",
    "autoencoderkl      = AutoencoderKL(**DAE_IMAGE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 32, 32])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_image                  = autoencoderkl.encode_stage_2_inputs(clean_image)\n",
    "latent_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 32, 32])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DAE_MASK_PARAMS        = {\"spatial_dims\"              : 2,\n",
    "                          \"in_channels\"               : 1,\n",
    "                          \"latent_channels\"           : 4, # (= Z in SDSeg paper)\n",
    "                          \"out_channels\"              : 1,\n",
    "                          \"channels\"                  : (128, 256, 512, 512),\n",
    "                          \"num_res_blocks\"            : 2,\n",
    "                          \"attention_levels\"          : (False, False, False, False),\n",
    "                          \"with_encoder_nonlocal_attn\": True, # (as per SDSeg paper to ensure middle block of encoder is as required)\n",
    "                          \"with_decoder_nonlocal_attn\": True, # (as per SDSeg paper to ensure middle block of decoder is as required)\n",
    "                          \"use_flash_attention\"       : True}\n",
    "\n",
    "dae_mask      = AutoencoderKL(**DAE_MASK_PARAMS)\n",
    "latent_mask   = dae_mask.encode_stage_2_inputs(clean_mask)\n",
    "latent_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDPMScheduler, DDIMScheduler\n",
    "from generative.inferers import LatentDiffusionInferer\n",
    "\n",
    "MODEL_PARAMS = {\"spatial_dims\"     : 2 if DIMENSION == \"2d\" else 3,\n",
    "                \"in_channels\"      : 8,  # Using latent space input (z = 4 + concatenation), so latent dimensions match autoencoder\n",
    "                \"out_channels\"     : 4,  # Latent space output before decoder\n",
    "                \"num_channels\"     : (192, 384, 384, 768, 768), # (192, 384, 384, 768, 768)\n",
    "                \"attention_levels\" : (True, True, True, True, True),\n",
    "                \"num_res_blocks\"   : 2,\n",
    "                \"num_head_channels\": 24} \n",
    "\n",
    "# Define model\n",
    "unet = DiffusionModelUNet(**MODEL_PARAMS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = (\n",
    "        DDIMScheduler(num_train_timesteps=NUM_TRAIN_TIMESTEPS, schedule=NOISE_SCHEDULER)\n",
    "        if SCHEDULER == \"DDIM\"\n",
    "        else DDPMScheduler(\n",
    "            num_train_timesteps=NUM_TRAIN_TIMESTEPS, schedule=NOISE_SCHEDULER\n",
    "        )\n",
    "    )\n",
    "inferer = LatentDiffusionInferer(scheduler=scheduler, scale_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_images               = autoencoderkl.encode_stage_2_inputs(clean_image).to('cpu')\n",
    "latent_masks                = dae_mask.encode_stage_2_inputs(clean_mask).to('cpu')\n",
    "noise                       = torch.randn_like(latent_masks).to('cpu') # (B, C, H, W)\n",
    "timesteps                   = torch.randint(0, scheduler.num_train_timesteps, (latent_masks.size(0),), device = 'cpu').long()\n",
    "z_T                         = scheduler.add_noise(original_samples = latent_masks, noise = noise, timesteps = timesteps)\n",
    "\n",
    "#[talha] Make sure z_t is same as the z_t we could have returned from inferer. \n",
    "noise_pred                  = inferer(inputs          = clean_mask,\n",
    "                                      noise             = noise,\n",
    "                                      diffusion_model   = unet,\n",
    "                                      timesteps         = timesteps,\n",
    "                                      autoencoder_model = dae_mask,\n",
    "                                      condition         = latent_images,\n",
    "                                      mode              = \"concat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 32, 32])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_noise             = F.l1_loss(noise_pred.float(), noise.float())\n",
    "            \n",
    "alpha_bar_T            = scheduler.alphas_cumprod[timesteps][:, None, None, None]\n",
    "z_0_pred               = (1 / torch.sqrt(alpha_bar_T)) * (z_T - (torch.sqrt(1 - alpha_bar_T) * noise_pred))\n",
    "loss_latent            = F.l1_loss(z_0_pred.float(), latent_masks.float())\n",
    "\n",
    "# Then add both losses and backpropogate.\n",
    "loss                   = loss_noise + loss_latent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
