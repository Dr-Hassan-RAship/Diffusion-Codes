{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 64, 32, 32])\n",
      "Output shape: torch.Size([2, 64, 32, 32])\n",
      "Attention weights shape: torch.Size([2, 64])\n",
      "Sample attention weights for first sample:\n",
      "tensor([0.4984, 0.4993, 0.5014, 0.4994, 0.4983])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define SELayer (as provided)\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# Create a sample input tensor\n",
    "batch_size, channels, height, width = 2, 64, 32, 32\n",
    "x = torch.randn(batch_size, channels, height, width)  # Simulated feature map\n",
    "\n",
    "# Instantiate SELayer\n",
    "se_layer = SELayer(channel=channels, reduction=16)\n",
    "\n",
    "# Forward pass\n",
    "output = se_layer(x)\n",
    "\n",
    "# Print shapes and sample attention weights\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Extract attention weights for inspection\n",
    "with torch.no_grad():\n",
    "    y = se_layer.avg_pool(x).view(batch_size, channels)\n",
    "    y = se_layer.fc(y)\n",
    "    print(f\"Attention weights shape: {y.shape}\")\n",
    "    print(f\"Sample attention weights for first sample:\\n{y[0][:5]}\")  # First 5 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 64, 32, 32])\n",
      "Output shape: torch.Size([2, 64, 32, 32])\n",
      "Attention map shape: torch.Size([2, 1024, 256])\n",
      "Sample attention weights (first 5x5 for first sample):\n",
      "tensor([[0.0052, 0.0056, 0.0017, 0.0021, 0.0009],\n",
      "        [0.0016, 0.0070, 0.0040, 0.0005, 0.0004],\n",
      "        [0.0097, 0.0216, 0.0051, 0.0020, 0.0051],\n",
      "        [0.0087, 0.0036, 0.0016, 0.0019, 0.0107],\n",
      "        [0.0001, 0.0020, 0.0030, 0.0100, 0.0002]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define NonLocalBlock (as provided)\n",
    "class NonLocalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
    "        super(NonLocalBlock, self).__init__()\n",
    "        self.sub_sample = sub_sample\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "        self.g = nn.Conv2d(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "        if bn_layer:\n",
    "            self.W = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                          kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(self.in_channels)\n",
    "            )\n",
    "            nn.init.constant_(self.W[1].weight, 0)\n",
    "            nn.init.constant_(self.W[1].bias, 0)\n",
    "        else:\n",
    "            self.W = nn.Conv2d(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                               kernel_size=1, stride=1, padding=0)\n",
    "            nn.init.constant_(self.W.weight, 0)\n",
    "            nn.init.constant_(self.W.bias, 0)\n",
    "        self.theta = nn.Conv2d(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                               kernel_size=1, stride=1, padding=0)\n",
    "        self.phi = nn.Conv2d(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "        if sub_sample:\n",
    "            self.g = nn.Sequential(self.g, nn.MaxPool2d(kernel_size=(2, 2)))\n",
    "            self.phi = nn.Sequential(self.phi, nn.MaxPool2d(kernel_size=(2, 2)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1)\n",
    "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "        f = torch.matmul(theta_x, phi_x)\n",
    "        f_div_C = F.softmax(f, dim=-1)\n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x\n",
    "        return z\n",
    "\n",
    "# Create a sample input tensor\n",
    "batch_size, in_channels, height, width = 2, 64, 32, 32\n",
    "x = torch.randn(batch_size, in_channels, height, width)  # Simulated feature map\n",
    "\n",
    "# Instantiate NonLocalBlock\n",
    "non_local_block = NonLocalBlock(in_channels=in_channels, inter_channels=None, sub_sample=True, bn_layer=True)\n",
    "\n",
    "# Forward pass\n",
    "output = non_local_block(x)\n",
    "\n",
    "# Print shapes and sample attention weights\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Extract attention map for inspection\n",
    "with torch.no_grad():\n",
    "    g_x = non_local_block.g(x).view(batch_size, non_local_block.inter_channels, -1)\n",
    "    theta_x = non_local_block.theta(x).view(batch_size, non_local_block.inter_channels, -1).permute(0, 2, 1)\n",
    "    phi_x = non_local_block.phi(x).view(batch_size, non_local_block.inter_channels, -1)\n",
    "    f = torch.matmul(theta_x, phi_x)\n",
    "    f_div_C = F.softmax(f, dim=-1)\n",
    "    print(f\"Attention map shape: {f_div_C.shape}\")\n",
    "    print(f\"Sample attention weights (first 5x5 for first sample):\\n{f_div_C[0, :5, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1.shape: torch.Size([1, 64, 176, 176]), x2.shape: torch.Size([1, 64, 88, 88]), x3.shape: torch.Size([1, 64, 44, 44])\n",
      "x2.shape: torch.Size([1, 64, 176, 176])\n",
      "x3.shape: torch.Size([1, 64, 176, 176])\n",
      "feat.shape: torch.Size([1, 192, 176, 176])\n",
      "y1.shape: torch.Size([1, 192, 1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [4, 64, 1, 1], expected input[1, 192, 4, 4] to have 64 channels, but got 192 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m hppf = HPPF(in_channels=in_channels)\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m output = \u001b[43mhppf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Extract attention weights for inspection\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TaN2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TaN2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mHPPF.forward\u001b[39m\u001b[34m(self, x1, x2, x3)\u001b[39m\n\u001b[32m     32\u001b[39m y1 = \u001b[38;5;28mself\u001b[39m.avg(feat)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my1.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my1.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m y2 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (2, 192, 4, 4)\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my2.shape before reshape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my2.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m y3 = \u001b[38;5;28mself\u001b[39m.conv2(\u001b[38;5;28mself\u001b[39m.max2(feat))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TaN2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TaN2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TaN2/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TaN2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TaN2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TaN2/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TaN2/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [4, 64, 1, 1], expected input[1, 192, 4, 4] to have 64 channels, but got 192 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define HPPF (as provided)\n",
    "class HPPF(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(HPPF, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, in_channels // 16, 1, 1), nn.ReLU(inplace=True))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels, in_channels // 64, 1, 1), nn.ReLU(inplace=True))\n",
    "        self.avg   = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max1  = nn.AdaptiveMaxPool2d(4)\n",
    "        self.max2  = nn.AdaptiveMaxPool2d(8)\n",
    "        self.mlp   = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 8, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // 8, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid())\n",
    "        self.feat_conv = nn.Sequential(nn.Conv2d(in_channels, in_channels // 3, 3, 1, 1),\n",
    "                                       nn.BatchNorm2d(in_channels // 3),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        print(f'x1.shape: {x1.shape}, x2.shape: {x2.shape}, x3.shape: {x3.shape}')\n",
    "        x2 = F.interpolate(x2, size=x1.size()[2:], mode='bilinear', align_corners=True)\n",
    "        print(f'x2.shape: {x2.shape}')\n",
    "        x3 = F.interpolate(x3, size=x1.size()[2:], mode='bilinear', align_corners=True)\n",
    "        print(f'x3.shape: {x3.shape}')\n",
    "        feat = torch.cat((x1, x2, x3), 1)\n",
    "        print(f'feat.shape: {feat.shape}')\n",
    "        b, c, h, w = feat.size()\n",
    "        y1 = self.avg(feat)\n",
    "        print(f\"y1.shape: {y1.shape}\")\n",
    "        y2 = self.conv1(self.max1(feat)) # (2, 192, 4, 4)\n",
    "        print(f\"y2.shape before reshape: {y2.shape}\")\n",
    "        y3 = self.conv2(self.max2(feat))\n",
    "        print(f\"y3.shape before reshape: {y3.shape}\")\n",
    "        y2 = y2.reshape(b, c, 1, 1)\n",
    "        print(f\"y2.shape after reshape: {y2.shape}\")\n",
    "        y3 = y3.reshape(b, c, 1, 1)\n",
    "        print(f\"y3.shape after reshape: {y3.shape}\")\n",
    "        z = (y1 + y2 + y3) // 3\n",
    "        print(f'z.shape: {z.shape}')\n",
    "        attention = self.mlp(z)\n",
    "        print(f\"attention.shape: {attention.shape}\")\n",
    "        output1 = attention * feat\n",
    "        print(f\"output1.shape: {output1.shape}\")\n",
    "        output2 = self.feat_conv(output1)\n",
    "        print(f'output2.shape: {output2.shape}')\n",
    "        return output2\n",
    "\n",
    "# Create sample input tensors\n",
    "\n",
    "batch_size, in_channels, height, width = 1, 64, 176, 176\n",
    "x1 = torch.randn(batch_size, in_channels, height, width)  # Feature map at highest resolution\n",
    "x2 = torch.randn(batch_size, in_channels, height//2, width//2)  # Lower resolution\n",
    "x3 = torch.randn(batch_size, in_channels, height//4, width//4)  # Lowest resolution\n",
    "\n",
    "# Instantiate HPPF\n",
    "hppf = HPPF(in_channels=in_channels)\n",
    "\n",
    "# Forward pass\n",
    "output = hppf(x1, x2, x3)\n",
    "\n",
    "# Extract attention weights for inspection\n",
    "with torch.no_grad():\n",
    "    x2_up = F.interpolate(x2, size=x1.size()[2:], mode='bilinear', align_corners=True)\n",
    "    x3_up = F.interpolate(x3, size=x1.size()[2:], mode='bilinear', align_corners=True)\n",
    "    feat = torch.cat((x1, x2_up, x3_up), 1)\n",
    "    y1 = hppf.avg(feat)\n",
    "    y2 = hppf.conv1(hppf.max1(feat)).reshape(batch_size, 3 * in_channels, 1, 1)\n",
    "    y3 = hppf.conv2(hppf.max2(feat)).reshape(batch_size, 3 * in_channels, 1, 1)\n",
    "    z = (y1 + y2 + y3) // 3\n",
    "    attention = hppf.mlp(z)\n",
    "    print(f\"Attention weights shape: {attention.shape}\")\n",
    "    print(f\"Sample attention weights (first 5 channels):\\n{attention[0, :5, 0, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[32m0.5\u001b[39m * torch.sum(\u001b[32m1\u001b[39m + logvar - mu.pow(\u001b[32m2\u001b[39m) - logvar.exp())\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m model = \u001b[43mViTVAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel.parameters())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mViTVAE.__init__\u001b[39m\u001b[34m(self, image_size, patch_size, hidden_size, num_layers, num_heads, mlp_size, latent_dim)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.pos_embed = nn.Parameter(torch.zeros(\u001b[32m1\u001b[39m, num_patches, hidden_size))\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Encoder Transformer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m encoder_layer = \u001b[43mTransformerEncoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_feedforward\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlp_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Latent layer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TaN2/lib/python3.11/site-packages/torch/nn/modules/transformer.py:712\u001b[39m, in \u001b[36mTransformerEncoderLayer.__init__\u001b[39m\u001b[34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, bias, device, dtype)\u001b[39m\n\u001b[32m    710\u001b[39m factory_kwargs = {\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m: device, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m: dtype}\n\u001b[32m    711\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m \u001b[38;5;28mself\u001b[39m.self_attn = \u001b[43mMultiheadAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;66;03m# Implementation of Feedforward model\u001b[39;00m\n\u001b[32m    721\u001b[39m \u001b[38;5;28mself\u001b[39m.linear1 = Linear(d_model, dim_feedforward, bias=bias, **factory_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TaN2/lib/python3.11/site-packages/torch/nn/modules/activation.py:1070\u001b[39m, in \u001b[36mMultiheadAttention.__init__\u001b[39m\u001b[34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[39m\n\u001b[32m   1068\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_first = batch_first\n\u001b[32m   1069\u001b[39m \u001b[38;5;28mself\u001b[39m.head_dim = embed_dim // num_heads\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m   1071\u001b[39m     \u001b[38;5;28mself\u001b[39m.head_dim * num_heads == \u001b[38;5;28mself\u001b[39m.embed_dim\n\u001b[32m   1072\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33membed_dim must be divisible by num_heads\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qkv_same_embed_dim:\n\u001b[32m   1075\u001b[39m     \u001b[38;5;28mself\u001b[39m.q_proj_weight = Parameter(\n\u001b[32m   1076\u001b[39m         torch.empty((embed_dim, embed_dim), **factory_kwargs)\n\u001b[32m   1077\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class ViTVAE(nn.Module):\n",
    "    def __init__(self, image_size=256, patch_size=16, hidden_size=512, num_layers=8, num_heads=12, mlp_size=2048, latent_dim=256):\n",
    "        super(ViTVAE, self).__init__()\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_size = patch_size\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(1, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size))\n",
    "        \n",
    "        # Encoder Transformer\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dim_feedforward=mlp_size)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Latent layer\n",
    "        self.latent_mu = nn.Linear(hidden_size, latent_dim)\n",
    "        self.latent_logvar = nn.Linear(hidden_size, latent_dim)\n",
    "        \n",
    "        # Decoder Transformer (simplified)\n",
    "        decoder_layer = TransformerEncoderLayer(d_model=latent_dim, nhead=num_heads, dim_feedforward=mlp_size)\n",
    "        self.decoder = TransformerEncoder(decoder_layer, num_layers=num_layers)\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches, latent_dim))\n",
    "        self.decoder_final = nn.ConvTranspose2d(latent_dim, 1, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        # x: [batch, 1, 256, 256]\n",
    "        x = self.patch_embed(x)  # [batch, hidden_size, 16, 16]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [batch, 256, hidden_size]\n",
    "        x = x + self.pos_embed\n",
    "        x = self.encoder(x)  # [batch, 256, hidden_size]\n",
    "        mu = self.latent_mu(x.mean(dim=1))  # [batch, latent_dim]\n",
    "        logvar = self.latent_logvar(x.mean(dim=1))  # [batch, latent_dim]\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = z.unsqueeze(1).repeat(1, 256, 1) + self.decoder_pos_embed  # [batch, 256, latent_dim]\n",
    "        z = self.decoder(z)  # [batch, 256, latent_dim]\n",
    "        z = z.transpose(1, 2).reshape(-1, latent_dim, 16, 16)  # [batch, latent_dim, 16, 16]\n",
    "        x_recon = self.decoder_final(z)  # [batch, 1, 256, 256]\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "    \n",
    "    def kl_loss(self, mu, logvar):\n",
    "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "# Example usage\n",
    "model = ViTVAE()\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TaN2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
