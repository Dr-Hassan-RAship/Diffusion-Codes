{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python312\\Scripts>\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torchinfo import summary\n",
    "from networks.novel.lip_resnet.lip_resnet_encoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_load = np.load('/Users/talhaahmed/Library/CloudStorage/OneDrive-HigherEducationCommission/Integration/GitHub/Diffusion-Codes/GMS/Dataset/busi/busi_train_test_names.pkl', allow_pickle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir = os.listdir('/Users/talhaahmed/Library/CloudStorage/OneDrive-HigherEducationCommission/Integration/GitHub/Diffusion-Codes/GMS/Dataset/busi/masks')\n",
    "len(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_rgb = torch.randn((1, 3, 224, 224))\n",
    "img_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable LIP params : 2.18 M\n",
      "Frozen backbone parms: 22.92 M\n"
     ]
    }
   ],
   "source": [
    "enc = LIPResNetEncoder(backbone = 'resnet50',   # or 'resnet34'\n",
    "                       pretrained      = False,\n",
    "                       latent_channels = 4,     # to match VAE Z_I\n",
    "                       model_freeze    = True,\n",
    "                       lip_freeze      = False)           # optional fine-tune flag\n",
    "feat_1_8, x, z_lip = enc(img_rgb)                          # shape (B, 4, 28, 28) if input 224×224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_1_8.shape: torch.Size([1, 512, 28, 28]), x.shape: torch.Size([1, 1000]), z_lip.shape: torch.Size([1, 4, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f'feat_1_8.shape: {feat_1_8.shape}, x.shape: {x.shape}, z_lip.shape: {z_lip.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #                   Trainable\n",
       "================================================================================================================================================================\n",
       "LIPResNetEncoder                                             [1, 3, 224, 224]          [1, 512, 28, 28]          --                        Partial\n",
       "├─_LIPResNetBackbone: 1-1                                    [1, 3, 224, 224]          [1, 512, 28, 28]          --                        Partial\n",
       "│    └─Conv2d: 2-1                                           [1, 3, 224, 224]          [1, 64, 112, 112]         (9,408)                   False\n",
       "│    └─BatchNorm2d: 2-2                                      [1, 64, 112, 112]         [1, 64, 112, 112]         (128)                     False\n",
       "│    └─ReLU: 2-3                                             [1, 64, 112, 112]         [1, 64, 112, 112]         --                        --\n",
       "│    └─SimplifiedLIP: 2-4                                    [1, 64, 112, 112]         [1, 64, 56, 56]           --                        True\n",
       "│    │    └─Sequential: 3-1                                  [1, 64, 112, 112]         [1, 64, 112, 112]         36,992                    True\n",
       "│    └─Sequential: 2-5                                       [1, 64, 56, 56]           [1, 256, 56, 56]          --                        False\n",
       "│    │    └─Bottleneck: 3-2                                  [1, 64, 56, 56]           [1, 256, 56, 56]          (75,008)                  False\n",
       "│    │    └─Bottleneck: 3-3                                  [1, 256, 56, 56]          [1, 256, 56, 56]          (70,400)                  False\n",
       "│    │    └─Bottleneck: 3-4                                  [1, 256, 56, 56]          [1, 256, 56, 56]          (70,400)                  False\n",
       "│    └─Sequential: 2-6                                       [1, 256, 56, 56]          [1, 512, 28, 28]          --                        Partial\n",
       "│    │    └─Bottleneck: 3-5                                  [1, 256, 56, 56]          [1, 512, 28, 28]          824,064                   Partial\n",
       "│    │    └─Bottleneck: 3-6                                  [1, 512, 28, 28]          [1, 512, 28, 28]          (280,064)                 False\n",
       "│    │    └─Bottleneck: 3-7                                  [1, 512, 28, 28]          [1, 512, 28, 28]          (280,064)                 False\n",
       "│    │    └─Bottleneck: 3-8                                  [1, 512, 28, 28]          [1, 512, 28, 28]          (280,064)                 False\n",
       "│    └─Sequential: 2-7                                       [1, 512, 28, 28]          [1, 1024, 14, 14]         --                        Partial\n",
       "│    │    └─Bottleneck: 3-9                                  [1, 512, 28, 28]          [1, 1024, 14, 14]         1,695,744                 Partial\n",
       "│    │    └─Bottleneck: 3-10                                 [1, 1024, 14, 14]         [1, 1024, 14, 14]         (1,117,184)               False\n",
       "│    │    └─Bottleneck: 3-11                                 [1, 1024, 14, 14]         [1, 1024, 14, 14]         (1,117,184)               False\n",
       "│    │    └─Bottleneck: 3-12                                 [1, 1024, 14, 14]         [1, 1024, 14, 14]         (1,117,184)               False\n",
       "│    │    └─Bottleneck: 3-13                                 [1, 1024, 14, 14]         [1, 1024, 14, 14]         (1,117,184)               False\n",
       "│    │    └─Bottleneck: 3-14                                 [1, 1024, 14, 14]         [1, 1024, 14, 14]         (1,117,184)               False\n",
       "│    └─Sequential: 2-8                                       [1, 1024, 14, 14]         [1, 2048, 7, 7]           --                        Partial\n",
       "│    │    └─Bottleneck: 3-15                                 [1, 1024, 14, 14]         [1, 2048, 7, 7]           4,913,664                 Partial\n",
       "│    │    └─Bottleneck: 3-16                                 [1, 2048, 7, 7]           [1, 2048, 7, 7]           (4,462,592)               False\n",
       "│    │    └─Bottleneck: 3-17                                 [1, 2048, 7, 7]           [1, 2048, 7, 7]           (4,462,592)               False\n",
       "│    └─AdaptiveAvgPool2d: 2-9                                [1, 2048, 7, 7]           [1, 2048, 1, 1]           --                        --\n",
       "│    └─Linear: 2-10                                          [1, 2048]                 [1, 1000]                 (2,049,000)               False\n",
       "├─Conv2d: 1-2                                                [1, 512, 28, 28]          [1, 4, 28, 28]            (2,052)                   False\n",
       "================================================================================================================================================================\n",
       "Total params: 25,098,156\n",
       "Trainable params: 2,175,104\n",
       "Non-trainable params: 22,923,052\n",
       "Total mult-adds (Units.GIGABYTES): 5.30\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 241.28\n",
       "Params size (MB): 95.47\n",
       "Estimated Total Size (MB): 337.35\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(enc, input_size=(1, 3, 224, 224), col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "# dummy tensors with requires_grad\n",
    "x     = torch.randn(2, 3, 16, 16, dtype=torch.double, requires_grad=True)\n",
    "logit = torch.randn(2, 1, 16, 16, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "def lip_wrapper(inp, lg):\n",
    "    w = lg.exp()\n",
    "    return F.avg_pool2d(inp * w, 3, 2, 1) / F.avg_pool2d(w, 3, 2, 1)\n",
    "\n",
    "# gradient check\n",
    "print(gradcheck(lip_wrapper, (x, logit), eps=1e-6, atol=1e-4))  # → True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------------------------------- LIP operator ------------------------------------ #\n",
    "def lip2d(x, logit, kernel=3, stride=2, padding=1, margin=1e-6):\n",
    "    weight = logit.exp()     # (B, 1, H, W), all values > 0\n",
    "    a = F.avg_pool2d(x * weight, kernel, stride, padding)   # x * weight --> weighted pooling \n",
    "    b = F.avg_pool2d(weight, kernel, stride, padding) + margin\n",
    "    return a / b             # normalized local weighted sum\n",
    "\n",
    "\n",
    "# ----------------------------------- Bottleneck Logit Module --------------------------- #\n",
    "class BottleneckLogit(nn.Module):\n",
    "    def __init__(self, in_channels, bottleneck_ratio=4):\n",
    "        super().__init__()\n",
    "        mid = in_channels // bottleneck_ratio\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid, 1),         # 1×1 conv: compression\n",
    "            nn.InstanceNorm2d(mid),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(mid, mid, 3, padding=1),       # 3×3 conv: spatial processing\n",
    "            nn.InstanceNorm2d(mid),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(mid, 1, 1)                     # 1×1 conv: compress to 1 channel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x) # logit output like a heatmap of importance of each pixel (x, y) regardless of channels\n",
    "\n",
    "# ----------------------------------- LIP Block ----------------------------------------- #\n",
    "class LIPBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, logit_module=None):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.logit_module = logit_module or BottleneckLogit(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        logits = self.logit_module(x)\n",
    "        return lip2d(x, logits)   # downsampling\n",
    "\n",
    "# ------------------------------ Arbitrary LIP Encoder ---------------------------------- #\n",
    "class ArbitraryLIPEngineEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, latent_channels=4):\n",
    "        super().__init__()\n",
    "        # Stem\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )  # (B, 64, 112, 112)\n",
    "\n",
    "        # LIP Block 1: (B, 64, 112, 112) → (B, 128, 56, 56)\n",
    "        self.block1 = LIPBlock(64, 128)\n",
    "\n",
    "        # LIP Block 2: (B, 128, 56, 56) → (B, 256, 28, 28)\n",
    "        self.block2 = LIPBlock(128, 256)\n",
    "\n",
    "        # Head: Project to latent space (B, 256, 28, 28) → (B, 4, 28, 28)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(256, 64, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, latent_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)     # (B, 64, 112, 112)\n",
    "        x = self.block1(x)   # (B, 128, 56, 56)\n",
    "        x = self.block2(x)   # (B, 256, 28, 28)\n",
    "        x = self.head(x)     # (B, 4, 28, 28)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ArbitraryLIPEngineEncoder(in_channels=3, latent_channels=4)\n",
    "x     = torch.randn(2, 3, 224, 224)\n",
    "z     = model(x)\n",
    "print(z.shape)  # ➜ torch.Size([2, 4, 28, 28])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(2, 3, 224, 224), col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.latent_mapping_model import *\n",
    "\n",
    "mapping_model = ResAttnUNet_DS(\n",
    "            in_channel = 4,\n",
    "            out_channels = 4,\n",
    "            num_res_blocks =  2,\n",
    "            ch = 32,\n",
    "            ch_mult= [1, 2, 4, 4],\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(mapping_model, input_size=(2, 4, 56, 56), col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For in_channels = 4 Trainable params = 1,564,976 vs for 8 channels its 1,566,128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def space_to_channel(x: torch.Tensor, p: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x : tensor of shape  (H, W, C)   (no batch dim shown)\n",
    "    p : spatial block size (must divide H and W)\n",
    "\n",
    "    returns: tensor of shape (H//p, W//p, p*p*C)\n",
    "    \"\"\"\n",
    "    x_orig = x\n",
    "\n",
    "    H, W, C = x.shape\n",
    "    assert H % p == 0 and W % p == 0, \"H and W must be divisible by p\"\n",
    "\n",
    "    # 1) split each spatial dim into (H//p, p) and (W//p, p)\n",
    "    x = x.view(H // p, p, W // p, p, C)          # (H/p, p, W/p, p, C)\n",
    "\n",
    "    # 2) bring the small p×p blocks next to the channel dim\n",
    "    x = x.permute(0, 2, 1, 3, 4)                 # (H/p, W/p, p, p, C)\n",
    "\n",
    "    # 3) merge them into the channel axis\n",
    "    x = x.reshape(H // p, W // p, C * p * p)     # (H/p, W/p, p²C)\n",
    "\n",
    "    y = rearrange(x_orig, '(h ph) (w pw) c -> h w (ph pw c)', ph=p, pw=p)\n",
    "\n",
    "    z = F.pixel_unshuffle(x_orig.permute(2, 0, 1), downscale_factor=p)\n",
    "    return x, y, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((224, 224, 3))\n",
    "p = 2\n",
    "\n",
    "x, y, z = space_to_channel(x, p = 2)\n",
    "\n",
    "print(f'x.shape: {x.shape}, y.shape: {y.shape}, z.shape: {z.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "def downsample_shortcut(x: torch.Tensor, p: int = 2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Non-learnable “space-to-channel + channel-averaging”\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    x : (H, W, C)  – no batch for brevity\n",
    "    p : block size (p = 2 in the diagram)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y : (H//p, W//p, 2C)\n",
    "    \"\"\"                                                          \n",
    "    H, W, C = x.shape\n",
    "    assert H % p == 0 and W % p == 0, \"H and W must be divisible by p\" \n",
    "\n",
    "    # 1) space → channel  … (H, W, C) ⟶ (H/p, W/p, p²·C)\n",
    "    s2c = rearrange(x, '(h ph) (w pw) c -> h w (ph pw c)', ph=p, pw=p) # H , w p, \n",
    "\n",
    "    # 2) split the p²·C channels into two equal parts and average them\n",
    "    g1, g2 = torch.chunk(s2c, 2, dim=-1)            # each: (H/p, W/p, p²·C/2)\n",
    "    y = 0.5 * (g1 + g2)                             # (H/p, W/p, 2C)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_shortcut(x: torch.Tensor, p: int = 2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Non-learnable “channel-to-space + channel-duplication”\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    x : (H//p, W//p, 2C)\n",
    "    p : block size (2 in the diagram)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y : (H, W, C)\n",
    "    \"\"\"\n",
    "    Hp, Wp, twoC = x.shape\n",
    "    assert twoC % (p**2) == 0, \"Channel dim must equal p² × (C/2)\"\n",
    "    C_half = twoC // (p**2)          # will become C/2 after expansion\n",
    "\n",
    "    # 1) channel → space  … (Hp, Wp, 2C) ⟶ (H, W, C/2)\n",
    "    c2s = rearrange(x, 'h w (ph pw c) -> (h ph) (w pw) c', ph=p, pw=p)\n",
    "\n",
    "    # 2) duplicate along channel dim and concatenate  … (H, W, C/2) ⟶ (H, W, C)\n",
    "    y = torch.cat([c2s, c2s], dim=-1)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, C = 224, 224, 4        # C must be even for the round-trip to work\n",
    "x0     = torch.randn(H, W, C)\n",
    "\n",
    "x1 = downsample_shortcut(x0)  # (112, 112, 8)\n",
    "x2 = upsample_shortcut(x1)    # (224, 224, 4)\n",
    "\n",
    "assert x2.shape == x0.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1) your original non-parametric helpers (unchanged, pasted for completeness)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def downsample_shortcut(x: torch.Tensor, p: int = 2) -> torch.Tensor:\n",
    "    H, W, C = x.shape\n",
    "    assert H % p == 0 and W % p == 0\n",
    "    s2c = rearrange(x, '(h ph) (w pw) c -> h w (ph pw c)', ph=p, pw=p)\n",
    "    g1, g2 = torch.chunk(s2c, 2, dim=-1)\n",
    "    return 0.5 * (g1 + g2)                       # (H/p, W/p, 2C)\n",
    "\n",
    "\n",
    "def upsample_shortcut(x: torch.Tensor, p: int = 2) -> torch.Tensor:\n",
    "    Hp, Wp, twoC = x.shape\n",
    "    assert twoC % (p**2) == 0\n",
    "    c2s = rearrange(x, 'h w (ph pw c) -> (h ph) (w pw) c', ph=p, pw=p)\n",
    "    return torch.cat([c2s, c2s], dim=-1)         # (H, W, C)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2) thin wrappers that make the helpers *batch-aware*\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def _map_per_sample(fn, x, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply `fn` to each sample in a batched tensor.\n",
    "    Accepts tensors of shape (B, H, W, C) or (H, W, C).\n",
    "    \"\"\"\n",
    "    if x.ndim == 3:   # (H, W, C)\n",
    "        return fn(x, **kwargs)\n",
    "    elif x.ndim == 4: # (B, H, W, C)\n",
    "        return torch.stack([fn(sample, **kwargs) for sample in x], dim=0)\n",
    "    else:\n",
    "        raise ValueError(\"Expected tensor of shape (H,W,C) or (B,H,W,C)\")\n",
    "\n",
    "def downsample_shortcut_batched(x, p=2):\n",
    "    return _map_per_sample(downsample_shortcut, x, p=p)\n",
    "\n",
    "def upsample_shortcut_batched(x, p=2):\n",
    "    return _map_per_sample(upsample_shortcut, x, p=p)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3) residual-autoencoding blocks that call the (batched) shortcuts\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "class ResidualDownAE(nn.Module):\n",
    "    \"\"\"\n",
    "    x  ──► learnable_down ──► (+) ──► y\n",
    "     ╰─► non-param down-shortcut ─╯\n",
    "    \n",
    "    • Input : (B?, H, W, C)\n",
    "    • Output: (B?, H/2, W/2, 2C)\n",
    "    \"\"\"\n",
    "    def __init__(self, learnable_down: nn.Module, p: int = 2):\n",
    "        super().__init__()\n",
    "        self.learnable_down = learnable_down   # any module that halves H,W and doubles C\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.learnable_down(x) + downsample_shortcut_batched(x, p=self.p)\n",
    "\n",
    "\n",
    "class ResidualUpAE(nn.Module):\n",
    "    \"\"\"\n",
    "    x  ──► learnable_up ──► (+) ──► y\n",
    "     ╰─► non-param up-shortcut ─╯\n",
    "    \n",
    "    • Input : (B?, H/2, W/2, 2C)\n",
    "    • Output: (B?, H,   W,   C)\n",
    "    \"\"\"\n",
    "    def __init__(self, learnable_up: nn.Module, p: int = 2):\n",
    "        super().__init__()\n",
    "        self.learnable_up = learnable_up       # any module that doubles H,W and halves C\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.learnable_up(x) + upsample_shortcut_batched(x, p=self.p)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4) minimal demo with toy learnable blocks\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "class ToyDown(nn.Module):     # (H,W,C) ➜ (H/2,W/2,2C)\n",
    "    def __init__(self, C): \n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(C, 2*C, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):      # convert to NCHW for conv, then back to HWC\n",
    "        x = self.conv(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "class ToyUp(nn.Module):       # (H/2,W/2,2C) ➜ (H,W,C)\n",
    "    def __init__(self, C): \n",
    "        super().__init__()\n",
    "        self.tconv = nn.ConvTranspose2d(2*C, C, 4, 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.tconv(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "# Instantiate ---------------------------------------------------------------\n",
    "B, H, W, C = 8, 256, 256, 64\n",
    "x = torch.randn(B, H, W, C)\n",
    "\n",
    "down_block = ResidualDownAE(ToyDown(C))\n",
    "up_block   = ResidualUpAE  (ToyUp(C))\n",
    "\n",
    "d = down_block(x)   # (B, 128, 128, 128)\n",
    "u = up_block(d)     # (B, 256, 256,  64)\n",
    "\n",
    "print(d.shape, u.shape)\n",
    "# -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# utils/load_tiny_vae.py\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from diffusers import AutoencoderTiny as HF_TinyVAE\n",
    "from networks.novel.tiny_vae.autoencoder_tiny import AutoencoderTiny  # your new class\n",
    "from torchinfo import summary\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Adjust these imports to match your repo layout\n",
    "# ------------------------------------------------------------------y\n",
    "\n",
    "def _remap_key(key: str) -> str:\n",
    "    # Only rename encoder downsample convs that are wrapped.\n",
    "    encoder_down_layers = {2, 6, 10}\n",
    "\n",
    "    enc_match = re.match(r\"(encoder\\.layers\\.(\\d+))\\.(weight|bias)\", key)\n",
    "    if enc_match:\n",
    "        idx = int(enc_match.group(2))\n",
    "        if idx in encoder_down_layers:\n",
    "            return f\"{enc_match.group(1)}.down.{enc_match.group(3)}\"\n",
    "        else:\n",
    "            return key\n",
    "\n",
    "    # Decoder: *no* renaming needed because convs are outside ResidualUpAE.\n",
    "    return key\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_residual_tiny_vae(\n",
    "    device=\"cuda\",\n",
    "    freeze=False,\n",
    "    dtype=torch.float32,\n",
    "):\n",
    "    # 1. Original checkpoint\n",
    "    hf_vae = HF_TinyVAE.from_pretrained(\"madebyollin/taesd\", torch_dtype=dtype)\n",
    "    hf_state = hf_vae.state_dict()\n",
    "\n",
    "    # 2. Remap keys\n",
    "    remapped = OrderedDict()\n",
    "    for k, v in hf_state.items():\n",
    "        remapped[_remap_key(k)] = v\n",
    "\n",
    "    # 3. Instantiate our new model\n",
    "    vae = AutoencoderTiny().to(device).to(dtype)\n",
    "\n",
    "    # 4. Load\n",
    "    missing, unexpected = vae.load_state_dict(remapped, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(\"⚠️  Unmatched keys\")\n",
    "        print(\"  missing   :\", missing)\n",
    "        print(\"  unexpected:\", unexpected)\n",
    "    else:\n",
    "        print(\"✅  State-dict loaded successfully.\")\n",
    "\n",
    "    # 5. Freeze if requested\n",
    "    if freeze:\n",
    "        for p in vae.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    vae.eval()\n",
    "    return vae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  State-dict loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "vae_model = load_residual_tiny_vae(device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape : torch.Size([2, 4, 28, 28])\n",
      "Decoder output shape : torch.Size([2, 3, 224, 224])\n",
      "Backward pass OK – grads exist: True\n",
      "\n",
      "— learnable parameters —\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Kernel Shape              Param #\n",
       "===============================================================================================\n",
       "AutoencoderTiny                               --                        --\n",
       "├─EncoderTiny: 1-1                            --                        --\n",
       "│    └─Sequential: 2-1                        --                        1,222,532\n",
       "├─DecoderTiny: 1-2                            --                        --\n",
       "│    └─Sequential: 2-2                        --                        1,222,531\n",
       "===============================================================================================\n",
       "Total params: 2,445,063\n",
       "Trainable params: 2,445,063\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 50.55\n",
       "===============================================================================================\n",
       "Input size (MB): 1.20\n",
       "Forward/backward pass size (MB): 750.68\n",
       "Params size (MB): 9.78\n",
       "Estimated Total Size (MB): 761.67\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------- hyper-params -----------------------------\n",
    "BATCH_SIZE   = 2\n",
    "IMG_SHAPE    = (3, 224, 224)   # C, H, W\n",
    "LATENT_CH    = 4               # matches default\n",
    "ENC_BLOCKS   = (1, 3, 3, 3)\n",
    "DEC_BLOCKS   = (3, 3, 3, 1)\n",
    "CHANNELS     = (64, 64, 64, 64)\n",
    "\n",
    "# ----------------------- instantiate model ------------------------\n",
    "# note that AutoencoderTiny default params is the same as the madebyollin/taesd config so the state dict can be easily loaded\n",
    "# ----------------------- forward pass -----------------------------\n",
    "x = torch.randn(BATCH_SIZE, *IMG_SHAPE)\n",
    "latents = vae_model.encode(x).latents\n",
    "print(\"Encoder output shape :\", latents.shape)          # ➞ (B, 4, 28, 28)\n",
    "\n",
    "recon = vae_model.decode(latents).sample\n",
    "print(\"Decoder output shape :\", recon.shape)            # ➞ (B, 3, 224, 224)\n",
    "\n",
    "# quick loss & backward\n",
    "loss = (recon - x).pow(2).mean()\n",
    "loss.backward()\n",
    "print(\"Backward pass OK – grads exist:\", vae_model.encoder.layers[0].weight.grad is not None)\n",
    "\n",
    "# ---------------------- parameter summary -------------------------\n",
    "print(\"\\n— learnable parameters —\")\n",
    "summary(vae_model, input_size = (BATCH_SIZE, *IMG_SHAPE), depth = 2, col_names = (\"kernel_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from networks.novel.DiffEIC.model import lfgcm_small\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = lfgcm_small.LFGCM(in_nc     = 3,\n",
    "                                out_nc    = 4,\n",
    "                                enc_mid   = [64, 128, 192, 192],\n",
    "                                N         = 192,\n",
    "                                M         = 320,\n",
    "                                prior_nc  = 64,\n",
    "                                sft_ks    = 3,\n",
    "                                slice_num = 10,\n",
    "                                slice_ch  = [8, 8, 8, 8, 16, 16, 32, 32, 96, 96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "LFGCM                                         [1, 4, 112, 112]          --\n",
       "├─Encoder: 1-1                                [1, 320, 56, 56]          --\n",
       "│    └─Sequential: 2-1                        [1, 64, 28, 28]           --\n",
       "│    │    └─Conv2d: 3-1                       [1, 64, 28, 28]           2,368\n",
       "│    │    └─GELU: 3-2                         [1, 64, 28, 28]           --\n",
       "│    │    └─Conv2d: 3-3                       [1, 64, 28, 28]           4,160\n",
       "│    └─Sequential: 2-2                        [1, 192, 112, 112]        --\n",
       "│    │    └─ResidualBlockWithStride: 3-4      [1, 192, 112, 112]        338,112\n",
       "│    │    └─ResidualBottleneck: 3-5           [1, 192, 112, 112]        120,192\n",
       "│    │    └─ResidualBottleneck: 3-6           [1, 192, 112, 112]        120,192\n",
       "│    │    └─ResidualBottleneck: 3-7           [1, 192, 112, 112]        120,192\n",
       "│    └─SFT: 2-3                               [1, 192, 112, 112]        --\n",
       "│    │    └─Sequential: 3-8                   [1, 128, 112, 112]        73,856\n",
       "│    │    └─Conv2d: 3-9                       [1, 192, 112, 112]        221,376\n",
       "│    │    └─Conv2d: 3-10                      [1, 192, 112, 112]        221,376\n",
       "│    └─Sequential: 2-4                        [1, 64, 14, 14]           --\n",
       "│    │    └─Conv2d: 3-11                      [1, 64, 14, 14]           36,928\n",
       "│    │    └─GELU: 3-12                        [1, 64, 14, 14]           --\n",
       "│    │    └─Conv2d: 3-13                      [1, 64, 14, 14]           4,160\n",
       "│    └─Sequential: 2-5                        [1, 192, 56, 56]          --\n",
       "│    │    └─ResidualBlockWithStride: 3-14     [1, 192, 56, 56]          700,992\n",
       "│    │    └─ResidualBottleneck: 3-15          [1, 192, 56, 56]          120,192\n",
       "│    │    └─ResidualBottleneck: 3-16          [1, 192, 56, 56]          120,192\n",
       "│    │    └─ResidualBottleneck: 3-17          [1, 192, 56, 56]          120,192\n",
       "│    └─SFT: 2-6                               [1, 192, 56, 56]          --\n",
       "│    │    └─Sequential: 3-18                  [1, 128, 56, 56]          73,856\n",
       "│    │    └─Conv2d: 3-19                      [1, 192, 56, 56]          221,376\n",
       "│    │    └─Conv2d: 3-20                      [1, 192, 56, 56]          221,376\n",
       "│    └─Sequential: 2-7                        [1, 64, 14, 14]           --\n",
       "│    │    └─Conv2d: 3-21                      [1, 64, 14, 14]           36,928\n",
       "│    │    └─GELU: 3-22                        [1, 64, 14, 14]           --\n",
       "│    │    └─Conv2d: 3-23                      [1, 64, 14, 14]           4,160\n",
       "│    └─Conv2d: 2-8                            [1, 192, 56, 56]          331,968\n",
       "│    └─SFTResblk: 2-9                         [1, 192, 56, 56]          --\n",
       "│    │    └─SFT: 3-24                         [1, 192, 56, 56]          516,608\n",
       "│    │    └─Conv2d: 3-25                      [1, 192, 56, 56]          331,968\n",
       "│    │    └─SFT: 3-26                         [1, 192, 56, 56]          516,608\n",
       "│    │    └─Conv2d: 3-27                      [1, 192, 56, 56]          331,968\n",
       "│    └─SFTResblk: 2-10                        [1, 192, 56, 56]          --\n",
       "│    │    └─SFT: 3-28                         [1, 192, 56, 56]          516,608\n",
       "│    │    └─Conv2d: 3-29                      [1, 192, 56, 56]          331,968\n",
       "│    │    └─SFT: 3-30                         [1, 192, 56, 56]          516,608\n",
       "│    │    └─Conv2d: 3-31                      [1, 192, 56, 56]          331,968\n",
       "│    └─Conv2d: 2-11                           [1, 320, 56, 56]          553,280\n",
       "├─Decoder: 1-2                                [1, 4, 112, 112]          --\n",
       "│    └─Sequential: 2-12                       [1, 192, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-32                      [1, 192, 56, 56]          553,152\n",
       "│    │    └─ResidualBottleneck: 3-33          [1, 192, 56, 56]          120,192\n",
       "│    │    └─ResidualBottleneck: 3-34          [1, 192, 56, 56]          120,192\n",
       "│    │    └─ResidualBottleneck: 3-35          [1, 192, 56, 56]          120,192\n",
       "│    └─Sequential: 2-13                       [1, 192, 112, 112]        --\n",
       "│    │    └─ResidualBlockUpsample: 3-36       [1, 192, 112, 112]        628,416\n",
       "│    │    └─ResidualBottleneck: 3-37          [1, 192, 112, 112]        120,192\n",
       "│    │    └─ResidualBottleneck: 3-38          [1, 192, 112, 112]        120,192\n",
       "│    │    └─ResidualBottleneck: 3-39          [1, 192, 112, 112]        120,192\n",
       "│    └─Conv2d: 2-14                           [1, 4, 112, 112]          6,916\n",
       "===============================================================================================\n",
       "Total params: 9,071,364\n",
       "Trainable params: 9,071,364\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 46.20\n",
       "===============================================================================================\n",
       "Input size (MB): 0.61\n",
       "Forward/backward pass size (MB): 573.21\n",
       "Params size (MB): 36.29\n",
       "Estimated Total Size (MB): 610.11\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_small, [(1, 3, 224, 224), (1, 4, 28, 28)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
