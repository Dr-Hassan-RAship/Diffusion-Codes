{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada94bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ee/anaconda3/envs/TaN2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Scripts Installation Path: 'C:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python312\\Scripts'\n",
    "\n",
    "# Libraries installed: \n",
    "# 1. diffusers\n",
    "# 2. transformers\n",
    "# 3. accelerate\n",
    "# 4. hf_xet\n",
    "\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch, transformers, shutil, os\n",
    "\n",
    "from diffusers                              import AutoencoderKL, UNet2DConditionModel, UNet2DModel, LMSDiscreteScheduler\n",
    "from transformers                           import CLIPTextModel, CLIPTokenizer\n",
    "from torchinfo                              import summary\n",
    "from PIL                                    import Image\n",
    "from torchvision                            import transforms as tfms\n",
    "from tqdm.auto                              import tqdm\n",
    "from IPython.display                        import display, clear_output\n",
    "from config                                 import *\n",
    "from architectures                          import *\n",
    "from pathlib                                import Path\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31bd908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DModel(**UNET_PARAMS).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(4, 8, 32, 32).to(dtype = torch.float16, device = device)\n",
    "mask = torch.randn(4, 8, 32, 32).to(dtype = torch.float16, device = device)\n",
    "t   = torch.randint(0, 1000, (4,), device=device).long()\n",
    "\n",
    "summary(unet, input_data = [inp, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96d96702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd1011596ad4d1e9242f40699acb746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Talha\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "## Initiating tokenizer and encoder.\n",
    "tokenizer    = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cpu\")\n",
    "\n",
    "## Initiating the VAE\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "## Initializing a scheduler and Setting number of sampling steps\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "scheduler.set_timesteps(50)\n",
    "\n",
    "## Initializing the U-Net model\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to run VAE (83.65 Million Params)\n",
    "\n",
    "# 1) Make sure input is dtype torch.float16 and on the same device as the model.\n",
    "# 2) The forward pass consists of 3 steps\n",
    "#    i)   posterior  = vae.encode(inputs).latent_dist --> gives a DiagnolGaussianDistribution Object which has the mean, logvar etc as its self members\n",
    "#    ii)  pos_sample = posterior.sample() --> simply returns a latent representation sample by x = self.mean + self.std * epsilon (of same shape as self.mean) with \n",
    "#         latent_dim = H/8, W /8\n",
    "#    iii) recon      = vae.decode(pos_sample).sample --> returns the reconstruction which is logits I believe.\n",
    "\n",
    "# Example Usage\n",
    "# >>> inputs = torch.randn(1, 3, 256, 256).to(torch.float16).to('cuda')\n",
    "# >>> recon  = vae(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646ab088",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(1, 3, 256, 256).to(torch.float16).to('cuda')\n",
    "posterior = vae.encode(inputs).latent_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e073c47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<diffusers.models.autoencoders.vae.DiagonalGaussianDistribution at 0x7f02942f2850>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de7b7211",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "def load_image(p):\n",
    "    '''\n",
    "    Function to load images from a defined path\n",
    "    '''\n",
    "    return Image.open(p).convert('RGB').resize((512,512))\n",
    "\n",
    "def pil_to_latents(image):\n",
    "    '''\n",
    "    Function to convert image to latents\n",
    "    '''\n",
    "    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n",
    "    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n",
    "    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n",
    "    return init_latent_dist\n",
    "\n",
    "def latents_to_pil(latents):\n",
    "    '''\n",
    "    Function to convert latents to images\n",
    "    '''\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images\n",
    "\n",
    "\n",
    "def text_enc(prompts, maxlen=None):\n",
    "    '''\n",
    "    A function to take a texual promt and convert it into embeddings\n",
    "    '''\n",
    "    if maxlen is None: maxlen = tokenizer.model_max_length\n",
    "    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n",
    "    return text_encoder(inp.input_ids.to(\"cpu\"))[0].half()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc70fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = 'A dog wearing a hat'\n",
    "# bs      = len(prompts) # 19\n",
    "# text    = text_enc(prompts) # (1, 77, 768) \n",
    "# uncond  = text_enc([''] * bs, text.shape[1]) # (19, 77, 768)\n",
    "# emb     = torch.cat([uncond, text])          # (20, 77, 768) \n",
    "\n",
    "# emb_filtered     = emb[:2, :, :]\n",
    "\n",
    "# print(f'text_encoding shape: {text.shape}, uncond_text.shape: {uncond.shape}, emb.shape: {emb.shape}, emb_filtered {emb_filtered.shape}')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a33b9b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 4, 64, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instanting random noise which will be noised and feeded to unet along with prompt. Note see 'https://huggingface.co/CompVis/stable-diffusion-v1-4/blob/main/unet/config.json' for the config for unet in SD 1.4\n",
    "dim = 512\n",
    "latents = torch.randn((bs, unet.config.in_channels, dim//8, dim//8))# Initiating random noise\n",
    "\n",
    "# Setting number of steps in scheduler\n",
    "scheduler.set_timesteps(70)\n",
    "\n",
    "# Adding noise to the latents \n",
    "latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_2_img(prompts, g=7.5, seed=100, steps=70, dim=512, save_int=True):\n",
    "    \"\"\"\n",
    "    Diffusion process to convert prompt to image\n",
    "    \"\"\"\n",
    "    \n",
    "    # Defining batch size\n",
    "    bs = len(prompts) \n",
    "    \n",
    "    # Converting textual prompts to embedding\n",
    "    text = text_enc(prompts) \n",
    "    \n",
    "    # Adding an unconditional prompt , helps in the generation process\n",
    "    uncond =  text_enc([\"\"] * bs, text.shape[1])\n",
    "    emb    = torch.cat([uncond, text])\n",
    "    \n",
    "    # Setting the seed\n",
    "    if seed: torch.manual_seed(seed)\n",
    "    \n",
    "    # Initiating random noise\n",
    "    latents = torch.randn((bs, unet.config.in_channels, dim//8, dim//8))\n",
    "    \n",
    "    # Setting number of steps in scheduler\n",
    "    scheduler.set_timesteps(steps)\n",
    "    \n",
    "    # Adding noise to the latents \n",
    "    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n",
    "\n",
    "    print(\"Processing text prompts:\", prompts)\n",
    "    # Just before the loop starts:\n",
    "    print(\"Visualizing initial latents...\")\n",
    "    latents_norm = torch.norm(latents.view(latents.shape[0], -1), dim=1).mean().item()\n",
    "    print(f\"Initial Latents Norm: {latents_norm}\")\n",
    "\n",
    "    # Iterating through defined steps\n",
    "    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n",
    "        # We need to scale the i/p latents to match the variance\n",
    "        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts) # [2 * B, 4, 32, 32]\n",
    "        \n",
    "        # Predicting noise residual using U-Net\n",
    "        print(f'ts: {ts}, inp.shape {inp.shape}, emb.shape {emb.shape}')\n",
    "        with torch.no_grad(): u, t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n",
    "            \n",
    "        # Performing Guidance\n",
    "        pred = u + g*(t-u)\n",
    "        \n",
    "        # Conditioning  the latents\n",
    "        latents = scheduler.step(pred, ts, latents).prev_sample\n",
    "        \n",
    "        # Inside your loop, after `latents` have been updated:\n",
    "        latents_norm = torch.norm(latents.view(latents.shape[0], -1), dim=1).mean().item()\n",
    "        print(f\"Step {i+1}/{steps} Latents Norm: {latents_norm}\")\n",
    "        \n",
    "        from IPython.display import display, clear_output\n",
    "        if   save_int and i%10==0: \n",
    "                image_path = f'steps2/la_{i:04d}.jpeg'\n",
    "                latents_to_pil(latents)[0].save(image_path)\n",
    "                display(latents_to_pil(latents)[0])  # Display the new image\n",
    "\n",
    "    return latents_to_pil(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c4e99df",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_2_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images:display(img)\n",
      "Cell \u001b[1;32mIn[13], line 26\u001b[0m, in \u001b[0;36mprompt_2_img\u001b[1;34m(prompts, g, seed, steps, dim, save_int)\u001b[0m\n\u001b[0;32m     23\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mset_timesteps(steps)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Adding noise to the latents \u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[43mlatents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhalf() \u001b[38;5;241m*\u001b[39m scheduler\u001b[38;5;241m.\u001b[39minit_noise_sigma\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing text prompts:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prompts)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Just before the loop starts:\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "images = prompt_2_img([prompts], save_int=True)\n",
    "for img in images:display(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1481d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
